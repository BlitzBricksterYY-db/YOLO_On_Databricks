{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8b9120-1b2e-4f47-85b8-26f4ee90bda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import serverless_gpu\n",
    "%pip install -U mlflow>=3.0\n",
    "%pip install threadpoolctl==3.1.0\n",
    "%pip install ultralytics==8.3.204\n",
    "%pip install nvidia-ml-py==13.580.82 # for later mlflow GPU monitoring\n",
    "%pip install pyrsmi==0.2.0 # for later mlflow AMD GPU monitoring if you have AMD\n",
    "\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4f83f4-ba4c-4aac-b4f2-3444f106388d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from serverless_gpu import distributed\n",
    "import mlflow\n",
    "\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from ultralytics.utils import RANK, LOCAL_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42531dbd-3ea0-4570-ba97-35dc3c03058d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%cat /tmp/Ultralytics/settings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c531b751-2f78-4dc6-96dc-309b003cb196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup I/O Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d14e30-c744-4232-bf5f-8ad34d9f26e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists yyang;\n",
    "create schema if not exists yyang.computer_vision;\n",
    "create volume if not exists yyang.computer_vision.yolo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c934f1e5-273f-4448-ad06-984749ce9d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_location = '/Volumes/yyang/computer_vision/yolo/'\n",
    "os.makedirs(f'{project_location}/training_runs/', exist_ok=True)\n",
    "os.makedirs(f'{project_location}/data/', exist_ok=True)\n",
    "os.makedirs(f'{project_location}/raw_model/', exist_ok=True)\n",
    "\n",
    "# volume folder in UC.\n",
    "volume_project_location = f'{project_location}/training_results/'\n",
    "os.makedirs(volume_project_location, exist_ok=True)\n",
    "\n",
    "# or alternatively, ephemeral /tmp/ project location on VM\n",
    "tmp_project_location = \"/tmp/training_results/\"\n",
    "os.makedirs(tmp_project_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb57faa-8cd4-4efe-bc23-88aebc793fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Image Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d365dc7c-dfda-4b2d-a892-551c92acad16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Workspace/' + dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().rsplit('/', 1)[0])\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d1e829-1767-401e-8fcb-6a405b2afb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# curl -L https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/datasets/coco8.yaml -o coco8.yaml\n",
    "curl -L https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/datasets/coco128.yaml -o coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9613b8d3-aa50-4c4b-90e6-a45581976941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# cat ./coco8.yaml\n",
    "cat ./coco128.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359206f2-de4f-4cc5-acb5-aae5bc8cfb90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "__REMEMBER: change below cell path for your data.yaml as input to YOLO train later__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e224cdc1-1bf0-46d7-9a59-a40642bf95b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# with open('coco8.yaml', 'r') as file:\n",
    "with open('coco128.yaml', 'r') as file:\n",
    "\n",
    "    data = file.read()\n",
    "\n",
    "#: here specific for this dataset, we have to update the .yaml file with real I/O locations.\n",
    "os.makedirs(f'{project_location}/data/coco128', exist_ok=True)\n",
    "data = data.replace('path: coco128', f'path: {project_location}data/coco128')\n",
    "\n",
    "\n",
    "# with open('coco8.yaml', 'w') as file:\n",
    "with open('coco128.yaml', 'w') as file:\n",
    "\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010c5888-7c1c-4bd2-8f2a-7ec5ed1787c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# cat ./coco8.yaml\n",
    "cat ./coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1c0f6-50c9-47bb-8610-89a13cda17fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# with open('./coco8.yaml', 'r') as file:\n",
    "with open('./coco128.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aef5597-e8f6-4bba-b01b-6b63424f114a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import tarfile\n",
    "# import io\n",
    "\n",
    "# response = requests.get(data['download'])\n",
    "# tar = tarfile.open(fileobj=io.BytesIO(response.content), mode='r:gz')\n",
    "# tar.extractall(path=data['path'])\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffda763-fe59-4d0c-9422-c42f9a0ffdea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Zip File from URL and Save to Path"
    }
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "response = requests.get(data['download'])\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "extraction_path = '/'.join(data['path'].split('/')[:-1]) # do this since we dont want to duplicate the \"/coco128/\" part twice in the final path.\n",
    "print(extraction_path)\n",
    "z.extractall(extraction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5636b3-8ede-46af-a89f-bd0ac0842ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7e6010-1b3a-465a-995a-a794541cc5d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "also move coco128.yaml file to there"
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# mv ./coco128.yaml /Volumes/yyang/computer_vision/yolo/data/\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "data_yaml_path = f'{extraction_path}/coco128.yaml'\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "if os.path.exists(data_yaml_path):\n",
    "    os.remove(data_yaml_path)\n",
    "shutil.move('./coco128.yaml', data_yaml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d829ce2-835a-4d9c-92fb-742c991e4a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup Mlflow Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da48cc45-99f1-4959-a96a-d1ab35382f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(\"string\", \"image_source\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(\"string\",\"class_name\"),\n",
    "                        ColSpec(\"integer\",\"class_num\"),\n",
    "                        ColSpec(\"double\",\"confidence\")]\n",
    "                       )\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, \n",
    "                           outputs=output_schema)\n",
    "\n",
    "# settings.update({\"mlflow\":False}) # Specifically, it disables the integration with MLflow. By setting the mlflow key to False, you are instructing the ultralytics library not to use MLflow for logging or tracking experiments.\n",
    "\n",
    "# ultralytics level setting with MLflow\n",
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "# # Config MLflow\n",
    "mlflow.autolog(disable=True)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659998a6-0118-4ca3-9e27-fab85a303cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_name = \"/Users/yang.yang@databricks.com/SGC_YOLO_Test/Experiments_YOLO_CoCo\"\n",
    "\n",
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "print(f\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING set to {os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING']}\")\n",
    "\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "print(f\"MLFLOW_EXPERIMENT_NAME set to {os.environ['MLFLOW_EXPERIMENT_NAME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7114dd4c-32ed-4f43-912f-a54f597e9879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Reset MLFLOW_RUN_ID, so we dont bump into the wrong one.\n",
    "if 'MLFLOW_RUN_ID' in os.environ:\n",
    "    del os.environ['MLFLOW_RUN_ID']\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "    active_run_id = mlflow.last_active_run().info.run_id\n",
    "    active_run_name = mlflow.last_active_run().info.run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9cb51d8-ee01-4a8f-acda-920d46ab7023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(experiment_name, experiment_id, active_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17a6ea3-d611-4f79-9546-463b7941191e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test if loadable"
    }
   },
   "outputs": [],
   "source": [
    "YOLO(\"yolo11n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76050ba2-5a74-4804-a5a7-98e607a17586",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "(optional) if you need to manually give the yaml path"
    }
   },
   "outputs": [],
   "source": [
    "# data_yaml_path = \"./coco128.yaml\" # ref: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml\n",
    "\n",
    "data_yaml_path = '/Volumes/yyang/computer_vision/yolo/data/coco128.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ce4204-8ab8-41ba-83f0-6ef73d992ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start to Train using SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9ac468-04cd-4bcb-8b2c-525a5a114bed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\"Initialize the distributed training process group\"\"\"\n",
    "    # Check if we're in a distributed environment\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        rank = int(os.environ['RANK'])\n",
    "        world_size = int(os.environ['WORLD_SIZE'])\n",
    "        local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    else:\n",
    "        # Fallback for single GPU\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "        local_rank = 0\n",
    "\n",
    "    # Initialize process group\n",
    "    if world_size > 1:\n",
    "        if not dist.is_initialized():\n",
    "            dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
    "\n",
    "    # Set device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f'cuda:{local_rank}')\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    return rank, world_size, device\n",
    "  \n",
    "def cleanup():\n",
    "    \"\"\"Clean up the distributed training process group\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d142d022-f643-4279-ac01-37626f5ba276",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "single local gpu test with minimal example"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "model.train(\n",
    "    task=\"detect\",\n",
    "    batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "    device=-1, # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "    data=data_yaml_path,\n",
    "    epochs=20,\n",
    "    project=f'{tmp_project_location}', # local VM ephermal location\n",
    "    # project=f'{volume_project_location}', # volume path still wont work\n",
    "    exist_ok=True,\n",
    "    fliplr=1,\n",
    "    flipud=1,\n",
    "    perspective=0.001,\n",
    "    degrees=.45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27e1bbb-d077-4dc4-95e4-d8676d97eb97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "working - v1a with 4 GPUs vs prior 8"
    }
   },
   "outputs": [],
   "source": [
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "mlflow.autolog(disable = False)\n",
    "\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "@distributed(gpus=4, gpu_type='A10', remote=True)\n",
    "#: -----------worker func: this function is visible to each GPU device.-------------------\n",
    "def train_fn(world_size = None, parent_run_id = None):\n",
    "\n",
    "\n",
    "    # import os\n",
    "    # from ultralytics import YOLO\n",
    "    # import torch\n",
    "    # import mlflow\n",
    "    # import torch.distributed as dist\n",
    "    # from ultralytics import settings\n",
    "    # from mlflow.types.schema import Schema, ColSpec\n",
    "    # from mlflow.models.signature import ModelSignature\n",
    "    from ultralytics.utils import RANK, LOCAL_RANK\n",
    "\n",
    "    # Setup distributed training\n",
    "    rank, world_size, device = setup()\n",
    "\n",
    "    print(f\"Rank: {rank}, World Size: {world_size}, Device: {device}\")\n",
    "    print(f\"Rank: {RANK}, World Size: {world_size}, Device: {LOCAL_RANK}\")\n",
    "\n",
    "\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "    ############################\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\" # use 1 for synchronization operation, debugging model prefers this.\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\" # \"WARN\" # for more debugging info on the NCCL side.\n",
    "    os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "    os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "    # We set the experiment details here\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # # #: from repo issue https://github.com/ultralytics/ultralytics/issues/11680\n",
    "    # ## conclusion: doesn't work, has error :\"ValueError: Invalid CUDA 'device=0,1' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\"\n",
    "    # # torch.backends.cudnn.benchmark = False\n",
    "    # # torch.cuda.synchronize()\n",
    "    # print(f\"------Before init_process_group, we have: {RANK=} -- {LOCAL_RANK=}------\")\n",
    "    # dist.init_process_group(\n",
    "    #     backend=\"nccl\",\n",
    "    #     init_method=\"env://\",\n",
    "    #     world_size=world_size,\n",
    "    #     rank=RANK, # this must be from 0 to world_size - 1. LOCAL_RANK wont work.\n",
    "    # )\n",
    "    # print(f\"------After init_process_group, we have: {RANK=} -- {LOCAL_RANK=}------\")\n",
    "\n",
    "    print('data_yaml_path is:', data_yaml_path)\n",
    "    #\n",
    "    # with mlflow.start_run(run_id=parent_run_id):\n",
    "    with mlflow.start_run():\n",
    "        # model = YOLO(f\"yolov11n.pt\") # shared location\n",
    "        model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "        model.train(\n",
    "            task=\"detect\",\n",
    "            batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "            device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "            data=data_yaml_path,\n",
    "            epochs=20,\n",
    "            project=f'{tmp_project_location}', # local VM ephermal location\n",
    "            # project=f'{volume_project_location}', # volume path still wont work\n",
    "            exist_ok=True,\n",
    "            fliplr=1,\n",
    "            flipud=1,\n",
    "            perspective=0.001,\n",
    "            degrees=.45\n",
    "        )\n",
    "        success = None\n",
    "        if RANK in (0, -1):\n",
    "            success = model.val()\n",
    "            if success:\n",
    "                model.export() # ref: https://docs.ultralytics.com/modes/export/#introduction\n",
    "        \n",
    "\n",
    "    active_run_id = mlflow.last_active_run().info.run_id\n",
    "    print(\"For YOLO autologging, active_run_id is: \", active_run_id)\n",
    "\n",
    "    # after training is done.\n",
    "    if not dist.is_initialized():\n",
    "      # import torch.distributed as dist\n",
    "      dist.init_process_group(\"nccl\")\n",
    "\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    global_rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    print(f\"------After training, we have: RANK:{global_rank=} -- LOCAL_RANK:{local_rank=} -- world_size: {world_size=}------\")\n",
    "\n",
    "    if global_rank == 0:\n",
    "        with mlflow.start_run(run_id=active_run_id) as run:\n",
    "            mlflow.log_artifact(data_yaml_path, \"input_data_yaml\")\n",
    "            # mlflow.log_dict(data, \"data.yaml\")\n",
    "            mlflow.log_params({\"rank\":global_rank})\n",
    "            mlflow.pytorch.log_model(YOLO(str(model.trainer.best)), \"model\", signature=signature) # this succeeded\n",
    "\n",
    "    # clean up\n",
    "    cleanup()\n",
    "\n",
    "    return \"finished\" # can return any picklable object\n",
    "\n",
    "\n",
    "train_fn.distributed(world_size = None, parent_run_id = None) # now can program can run without specifying manually the parameters of world_size and parent_run_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f734b7f-d542-4fff-9983-2fe8400cfcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note: I want to test more than 8 GPUs, but currently the dogfood has 8 A10 limitations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01870009-b544-4b31-a179-a2b66b9d3570",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "cleaned version - v2a with 8 GPUs vs prior 8 to test more nodes scalability"
    }
   },
   "outputs": [],
   "source": [
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "mlflow.autolog(disable = False)\n",
    "\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "#: -----------worker func: this function is visible to each GPU device.-------------------\n",
    "def train_fn(world_size = None, parent_run_id = None):\n",
    "    try:\n",
    "        from ultralytics.utils import RANK, LOCAL_RANK\n",
    "\n",
    "        # Setup distributed training\n",
    "        rank, world_size, device = setup()\n",
    "\n",
    "        print(f\"Rank: {rank}, World Size: {world_size}, Device: {device}\")\n",
    "        print(f\"Rank: {RANK}, World Size: {world_size}, Device: {LOCAL_RANK}\")\n",
    "\n",
    "\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "        ############################\n",
    "        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\" # use 1 for synchronization operation, debugging model prefers this.\n",
    "        os.environ[\"NCCL_DEBUG\"] = \"INFO\" # \"WARN\" # for more debugging info on the NCCL side.\n",
    "        os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "        os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "        # We set the experiment details here\n",
    "        experiment = mlflow.set_experiment(experiment_name)\n",
    "        print('data_yaml_path is:', data_yaml_path)\n",
    "        \n",
    "        #\n",
    "        # with mlflow.start_run(run_id=parent_run_id):\n",
    "        with mlflow.start_run():\n",
    "            model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "            model.train(\n",
    "                task=\"detect\",\n",
    "                batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "                device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "                data=data_yaml_path,\n",
    "                epochs=100,\n",
    "                project=f'{tmp_project_location}', # local VM ephermal location\n",
    "                # project=f'{volume_project_location}', # volume path still wont work\n",
    "                exist_ok=True,\n",
    "                fliplr=1,\n",
    "                flipud=1,\n",
    "                perspective=0.001,\n",
    "                degrees=.45\n",
    "            )\n",
    "            success = None\n",
    "            if RANK in (0, -1):\n",
    "                success = model.val()\n",
    "                if success:\n",
    "                    model.export() # ref: https://docs.ultralytics.com/modes/export/#introduction\n",
    "            \n",
    "\n",
    "        active_run_id = mlflow.last_active_run().info.run_id\n",
    "        print(\"For YOLO autologging, active_run_id is: \", active_run_id)\n",
    "\n",
    "        # after training is done.\n",
    "        if not dist.is_initialized():\n",
    "        # import torch.distributed as dist\n",
    "            dist.init_process_group(\"nccl\")\n",
    "\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        global_rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        print(f\"------After training, we have: RANK:{global_rank=} -- LOCAL_RANK:{local_rank=} -- world_size: {world_size=}------\")\n",
    "\n",
    "        if global_rank == 0:\n",
    "            with mlflow.start_run(run_id=active_run_id) as run:\n",
    "                mlflow.log_artifact(data_yaml_path, \"input_data_yaml\")\n",
    "                # mlflow.log_dict(data, \"data.yaml\")\n",
    "                mlflow.log_params({\"rank\":global_rank})\n",
    "                mlflow.pytorch.log_model(YOLO(str(model.trainer.best)), \"model\", signature=signature) # this succeeded\n",
    "                #: TODO: we can log more stuff here\n",
    "        \n",
    "        return \"finished\" # can return any picklable object\n",
    "    \n",
    "    finally:\n",
    "        # clean up\n",
    "        cleanup()\n",
    "\n",
    "\n",
    "train_fn.distributed(world_size = None, parent_run_id = None) # now can program can run without specifying manually the parameters of world_size and parent_run_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69cd261d-d018-41fe-aed5-204fcfc1fe2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Supplemental Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9621cf66-f949-4cb7-ad66-4305d4a68746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Tip 1\n",
    "For GPU resource not ready timeout error, consider to add these settings.\n",
    "\n",
    "error msg: \"torch.distributed.DistStoreError: Timed out after 601 seconds waiting for clients. 7/8 clients joined.\"\n",
    "\n",
    "```\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'\n",
    "\n",
    "import os\n",
    "os.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1'  # Recommended for better error reporting\n",
    "os.environ['NCCL_BLOCKING_WAIT'] = '1'         # Wait for full timeout\n",
    "os.environ['NCCL_SOCKET_TIMEOUT'] = '600'      # Set a socket timeout in seconds\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'              # Enable debug logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fc89a3-ec60-45d1-a3ae-421f6d931c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Overall Log Screening and Recommendations\n",
    "\n",
    "# Analysis of YOLO Training Log and Optimization Recommendations\n",
    "\n",
    "Based on the comprehensive analysis of your training log and research into distributed training best practices, here are the key areas for improvement in your cluster and training job configuration.\n",
    "\n",
    "## **Major Issues Identified**\n",
    "\n",
    "### **1. NCCL Network Communication Problems**\n",
    "\n",
    "The most significant issue in your log is the **failed NCCL network initialization**[1][2][3]. The errors show:\n",
    "\n",
    "- `NET/OFI aws-ofi-nccl initialization failed`\n",
    "- `NET/OFI Unable to find a protocol that worked`\n",
    "- `Using network Socket` (fallback to slower TCP networking)\n",
    "\n",
    "This means your distributed training is **falling back to slower Socket-based communication** instead of using optimized network fabrics, significantly reducing performance[4][5].\n",
    "\n",
    "### **2. Suboptimal Batch Size and Worker Configuration**\n",
    "\n",
    "Your current setup shows **8 dataloader workers** with an unspecified batch size. Research indicates this configuration may not be optimal for your 8-GPU setup[6][7][8].\n",
    "\n",
    "### **3. Databricks Serverless GPU Beta Limitations**\n",
    "\n",
    "The warning `serverless_gpu is in Beta. The API is subject to change` indicates you're using experimental infrastructure that may have performance and stability limitations[9][10].\n",
    "\n",
    "## **Cluster-Level Optimizations**\n",
    "\n",
    "### **Network Configuration**\n",
    "\n",
    "**Fix NCCL Network Issues:**\n",
    "- Set `NCCL_SOCKET_IFNAME=eth0` explicitly in your environment variables[3][11]\n",
    "- Add `NCCL_DEBUG=INFO` to get detailed networking information[12][11]\n",
    "- For AWS environments, ensure EFA (Elastic Fabric Adapter) is properly configured if available[4][13]\n",
    "\n",
    "**Recommended Environment Variables:**\n",
    "```bash\n",
    "export NCCL_SOCKET_IFNAME=eth0\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_NET=\"Socket\"  # Explicit fallback if EFA unavailable\n",
    "```\n",
    "\n",
    "### **Multi-GPU Setup Optimization**\n",
    "\n",
    "**Move from Serverless to Dedicated GPU Cluster:**\n",
    "Consider migrating from Databricks Serverless GPU (Beta) to a dedicated multi-GPU cluster for production training[14][15]. **Single-node multi-GPU setups typically outperform multi-node configurations** for YOLO training due to reduced network overhead[14].\n",
    "\n",
    "**Optimal Hardware Configuration:**\n",
    "- **Single node with 8 GPUs** is likely faster than 8 nodes with 1 GPU each[14]\n",
    "- Use **cluster placement groups** to minimize network latency[4]\n",
    "- Ensure all nodes have **identical PyTorch, CUDA, and NCCL versions**[16]\n",
    "\n",
    "## **Training Job Optimizations**\n",
    "\n",
    "### **Batch Size Optimization** \n",
    "\n",
    "**Use Automatic Batch Size Detection:**\n",
    "```python\n",
    "# Use batch=-1 for automatic optimal batch size calculation\n",
    "model.train(data=\"coco128.yaml\", epochs=100, batch=-1, device=[0,1,2,3,4,5,6,7])\n",
    "```\n",
    "\n",
    "This will automatically determine the **maximum batch size your GPUs can handle**[6][7], which is typically more efficient than manual guessing.\n",
    "\n",
    "**Manual Batch Size Guidelines:**\n",
    "- For 8 GPUs: Start with **batch=64** (8 per GPU) and scale up[6]\n",
    "- **Batch sizes of 16, 32, or 64 typically yield best results**[6]\n",
    "- Monitor GPU memory usage and increase until you approach memory limits[7]\n",
    "\n",
    "### **Dataloader Worker Optimization**\n",
    "\n",
    "**Reduce Worker Count:**\n",
    "Your current **8 workers may be excessive** for this setup[6][17]. Try:\n",
    "```python\n",
    "# Start with fewer workers to reduce RAM usage\n",
    "model.train(workers=4)  # or workers=2\n",
    "```\n",
    "\n",
    "**Memory Management:**\n",
    "- Disable image caching if experiencing high RAM usage: `cache=False`[17][18]\n",
    "- Monitor RAM usage during training - high worker counts can exhaust system memory[17]\n",
    "\n",
    "### **Training Parameters**\n",
    "\n",
    "**Enable Mixed Precision Training:**\n",
    "```python\n",
    "model.train(amp=True)  # Automatic Mixed Precision\n",
    "```\n",
    "This can **improve training speed and reduce memory usage** without sacrificing accuracy[7].\n",
    "\n",
    "**Optimize Image Processing:**\n",
    "```python\n",
    "model.train(\n",
    "    data=\"coco128.yaml\",\n",
    "    epochs=100, \n",
    "    batch=-1,           # Auto-detect optimal batch size\n",
    "    workers=4,          # Reduced worker count\n",
    "    device=[0,1,2,3,4,5,6,7],  # All 8 GPUs\n",
    "    amp=True,           # Mixed precision\n",
    "    cache=False         # Disable caching if RAM limited\n",
    ")\n",
    "```\n",
    "\n",
    "## **Monitoring and Debugging**\n",
    "\n",
    "### **Performance Monitoring**\n",
    "\n",
    "**Add NCCL Debugging:**\n",
    "Set `NCCL_DEBUG=INFO` to monitor network communication efficiency[3][11]. Look for:\n",
    "- Successful network initialization messages\n",
    "- Bandwidth utilization statistics\n",
    "- Communication pattern optimization\n",
    "\n",
    "**Track Key Metrics:**\n",
    "- **GPU utilization** (should be >90% during training)\n",
    "- **Network bandwidth utilization**\n",
    "- **Memory usage** (both GPU and system RAM)\n",
    "- **Training iteration time** and consistency[14]\n",
    "\n",
    "### **Troubleshooting Steps**\n",
    "\n",
    "1. **Test NCCL Communication:**\n",
    "   ```bash\n",
    "   # Run NCCL tests to verify network performance\n",
    "   python -c \"import torch; torch.distributed.init_process_group('nccl')\"\n",
    "   ```\n",
    "\n",
    "2. **Verify GPU Topology:**\n",
    "   Check GPU interconnects and ensure optimal placement[19]\n",
    "\n",
    "3. **Monitor Resource Usage:**\n",
    "   Use Databricks cluster metrics to identify bottlenecks[14]\n",
    "\n",
    "## **Long-term Recommendations**\n",
    "\n",
    "### **Infrastructure Migration**\n",
    "\n",
    "**Consider Moving to Production Infrastructure:**\n",
    "- Migrate from **Serverless GPU (Beta)** to stable, dedicated GPU clusters[9]\n",
    "- Use **instance types optimized for ML workloads** (e.g., p3, p4, g4 instances on AWS)[4]\n",
    "- Implement **proper EFA networking** for multi-node scenarios[20][4]\n",
    "\n",
    "### **Training Strategy**\n",
    "\n",
    "**Implement Progressive Training:**\n",
    "- Start with **smaller models and datasets** for parameter tuning\n",
    "- Use **gradient accumulation** if memory constraints limit batch size[7]\n",
    "- Consider **staged training** (train for shorter epochs, then resume) to avoid memory accumulation issues[17]\n",
    "\n",
    "The primary bottleneck in your current setup appears to be the **failed network optimization and suboptimal batch/worker configuration**. Addressing the NCCL networking issues should provide the most significant performance improvement, followed by optimizing batch size and reducing the worker count to prevent memory exhaustion.\n",
    "\n",
    "Sources\n",
    "[1] Slow NCCL gradient synchronization in distributed training https://discuss.pytorch.org/t/slow-nccl-gradient-synchronization-in-distributed-training/89625\n",
    "[2] Model Training with Ultralytics YOLO https://docs.ultralytics.com/modes/train/\n",
    "[3] NCCL Ignores Specified SOCKET_IFNAME Configuration ... - GitHub https://github.com/NVIDIA/nccl/issues/1581\n",
    "[4] Optimizing deep learning on P3 and P3dn with EFA - AWS https://aws.amazon.com/blogs/compute/optimizing-deep-learning-on-p3-and-p3dn-with-efa/\n",
    "[5] NCCL performance for Deep Learning workloads on AWS EFA ... https://github.com/NVIDIA/nccl/issues/235\n",
    "[6] What's an efficient way to fine tune the batch size? #3572 - GitHub https://github.com/ultralytics/ultralytics/issues/3572\n",
    "[7] Machine Learning Best Practices and Tips for Model Training https://docs.ultralytics.com/guides/model-training-tips/\n",
    "[8] I am seeing major improvements in my model and the only change ... https://community.ultralytics.com/t/i-am-seeing-major-improvements-in-my-model-and-the-only-change-has-been-the-machine-it-is-trained-on/1019\n",
    "[9] Serverless GPU compute | Databricks on AWS https://docs.databricks.com/aws/en/compute/serverless/gpu\n",
    "[10] Serverless GPU compute - Azure Databricks - Microsoft Learn https://learn.microsoft.com/en-us/azure/databricks/compute/serverless/gpu\n",
    "[11] How to set NCCL_SOCKET_IFNAME · Issue #286 · NVIDIA/nccl https://github.com/NVIDIA/nccl/issues/286\n",
    "[12] NCCL - CSCS Documentation https://docs.cscs.ch/software/communication/nccl/\n",
    "[13] Optimizing deep learning on P3 and P3dn with EFA - AWS https://aws.amazon.com/blogs/compute/optimizing-deep-learning-on-p3-and-p3dn-with-efa-part-1/\n",
    "[14] Best practices for deep learning on Databricks https://docs.databricks.com/aws/en/machine-learning/train-model/dl-best-practices\n",
    "[15] Multi-GPU and multi-node distributed training | Databricks on AWS https://docs.databricks.com/aws/en/machine-learning/sgc-examples/gpu-distributed-training\n",
    "[16] Multi node training of YOLOv8 (2 machine with 4GPU each) #7038 https://github.com/ultralytics/ultralytics/issues/7038\n",
    "[17] High RAM utilization during training - PyTorch Forums https://discuss.pytorch.org/t/high-ram-utilization-during-training/159939\n",
    "[18] how to avoid high RAM usage · Issue #1467 - GitHub https://github.com/ultralytics/ultralytics/issues/1467\n",
    "[19] Distributed Parallel Training: PyTorch Multi-GPU Setup in Kaggle T4x2 https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/\n",
    "[20] Get started with EFA and NCCL for ML workloads on Amazon EC2 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start-nccl.html\n",
    "[21] DDP: multi node training · Issue #6286 - GitHub https://github.com/ultralytics/ultralytics/issues/6286\n",
    "[22] Multi-GPU Training with YOLOv5 - Ultralytics YOLO Docs https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/\n",
    "[23] Neuron Runtime Troubleshooting on Inf1, Inf2 and Trn1 https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html\n",
    "[24] Enabling Fast Inference and Resilient Training with NCCL 2.27 https://developer.nvidia.com/blog/enabling-fast-inference-and-resilient-training-with-nccl-2-27/\n",
    "[25] How to train yolov8 with multi-gpu? · Issue #3308 - GitHub https://github.com/ultralytics/ultralytics/issues/3308\n",
    "[26] [bug] NCCL WARN NET/OFI Only EFA provider is supported #2675 https://github.com/aws/deep-learning-containers/issues/2675\n",
    "[27] Issues when trying to train on a multi-GPU device #5244 - GitHub https://github.com/ultralytics/ultralytics/issues/5244\n",
    "[28] Version · Issue #391 · aws/aws-ofi-nccl - GitHub https://github.com/aws/aws-ofi-nccl/issues/391\n",
    "[29] Distributed Training: Definition & How it Works - Ultralytics https://www.ultralytics.com/glossary/distributed-training\n",
    "[30] Using EFA on the DLAMI - AWS Deep Learning AMIs https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa-using.html\n",
    "[31] On the Performance and Memory Footprint of Distributed Training https://arxiv.org/html/2407.02081v1\n",
    "[32] YOLO v11 training multi-GPU DDP Errors - Stack Overflow https://stackoverflow.com/questions/79372969/yolo-v11-training-multi-gpu-ddp-errors\n",
    "[33] Distributed Parallel Training Example (GPU) https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html\n",
    "[34] Configure YOLOv8 for GPU: Accelerate Object Detection https://www.digitalocean.com/community/tutorials/yolov8-for-gpu-accelerate-object-detection\n",
    "[35] Simplifying Training and GenAI Finetuning Using Serverless GPU ... https://www.youtube.com/watch?v=pQMeeQ_jGY0\n",
    "[36] aws-samples/eks-efa-examples - GitHub https://github.com/aws-samples/eks-efa-examples\n",
    "[37] Multi-GPU and multi-node distributed training - Azure Databricks https://learn.microsoft.com/en-us/azure/databricks/machine-learning/sgc-examples/gpu-distributed-training\n",
    "[38] Configuration - Ultralytics YOLO Docs https://docs.ultralytics.com/usage/cfg/\n",
    "[39] Best practices for performance efficiency | Databricks on AWS https://docs.databricks.com/aws/en/lakehouse-architecture/performance-efficiency/best-practices\n",
    "[40] YOLOv5 Study: mAP vs Batch-Size #2452 - GitHub https://github.com/ultralytics/yolov5/discussions/2452\n",
    "[41] High-Performance GPU Memory Transfer on AWS Sagemaker ... https://www.perplexity.ai/hub/blog/high-performance-gpu-memory-transfer-on-aws\n",
    "[42] ML Training Tip Of The Week #1: Optimizing GPU ... - 86677 https://community.databricks.com/t5/technical-blog/ml-training-tip-of-the-week-1-optimizing-gpu-utilization-in/ba-p/86677\n",
    "[43] Tips for Best YOLOv5 Training Results - Ultralytics YOLO Docs https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/\n",
    "[44] NCCL error when using Sagemaker distributed training without ... https://stackoverflow.com/questions/75064559/nccl-error-when-using-sagemaker-distributed-training-without-specifying-a-distri\n",
    "[45] Normal then slow then crashing training - YOLO - Ultralytics https://community.ultralytics.com/t/normal-then-slow-then-crashing-training/1203\n",
    "[46] The usage of video memory fluctuates greatly during YOLO11 training https://github.com/ultralytics/ultralytics/issues/20860\n",
    "[47] Serverless compute plane networking - Azure Databricks https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/\n",
    "[48] Unable to see NCCL logs - PyTorch Forums https://discuss.pytorch.org/t/unable-to-see-nccl-logs/176114\n",
    "[49] Optimize GPU utilization while training - YOLO - Ultralytics https://community.ultralytics.com/t/optimize-gpu-utilization-while-training/768\n",
    "[50] https://raw.githubusercontent.com/aws-samples/awso... https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/efa-cheatsheet.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acaa5bb-248c-4467-bae0-b86858c71f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from ultralytics.utils import RANK, LOCAL_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b3b98d-bfc0-45b0-b06d-86e4e7cbcdc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Minimal_example_v"
    }
   },
   "outputs": [],
   "source": [
    "# data_yaml_path = \"coco128.yaml\" # ref: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml\n",
    "\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "def train_fn():\n",
    "  # Start a run to represent the training job\n",
    "  with mlflow.start_run():\n",
    "    model = YOLO(f\"yolo11n\") # shared location\n",
    "    # model = YOLO(\"yolo11n\")\n",
    "    model.train(\n",
    "        task=\"detect\",\n",
    "        batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "        device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "        data=data_yaml_path,\n",
    "        epochs=100,\n",
    "        project=f'{tmp_project_location}', # local VM ephermal location\n",
    "        # project=f'{volume_project_location}', # volume path still wont work\n",
    "        exist_ok=True,\n",
    "        fliplr=1,\n",
    "        flipud=1,\n",
    "        perspective=0.001,\n",
    "        degrees=.45\n",
    "    )\n",
    "\n",
    "train_fn.distributed()   \n",
    "\n",
    "## : conclusion\n",
    "## after a few iterations (with screenshots stored locally for error msgs from experiment log), we conclude it wont work for @distributed with simple setup.\n",
    "\n",
    "\n",
    "#: ----comment below out cause it was for classical GPU compute.----\n",
    "# distributor = TorchDistributor(num_processes=1, local_mode=True, use_gpu=True)      \n",
    "# distributor.run(train_fn)\n",
    "# # on sgc, error: [CONFIG_NOT_AVAILABLE] Configuration spark.master is not available. SQLSTATE: 42K0I"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6482896598264492,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_quick test yolo11n with coco128 on SGC A10 Remote",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
